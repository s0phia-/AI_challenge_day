{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s0phia-/AI_challenge_day/blob/main/nondeterminism_in_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implications of Non-Determinism in Neural Network Optimisation\n",
        "\n",
        "Description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Environment Setup ###\n",
        "\n",
        "%pip install -q torch torchvision\n",
        "%pip install matplotlib\n",
        "%pip install numpy\n",
        "%pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6iv79F_F3iq",
        "outputId": "4f768ab2-a85b-4365-cd2f-4185ea7a26aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:01<00:00, 106MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#### Load Data ###\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',\n",
        "           'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dwb6ADcjYqvM"
      },
      "outputs": [],
      "source": [
        "### Define ResNet model ###\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "\n",
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "  \"\"\"3x3 convolution with padding\"\"\"\n",
        "  return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride,\n",
        "      padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    # Both self.conv1 and self.downsample layers downsample the input when\n",
        "    # stride != 1\n",
        "    self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.conv2 = conv3x3(planes, planes)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.downsample = downsample\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = x\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)\n",
        "      identity = torch.cat((identity, torch.zeros_like(identity)), 1)\n",
        "\n",
        "    out += identity\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "  def __init__(self, block, layers, channels_per_block=None, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "\n",
        "    if channels_per_block is None:\n",
        "      channels_per_block = [16, 32, 64]\n",
        "    if len(layers) != len(channels_per_block):\n",
        "      raise ValueError('number of layers and channels per block must be equal')\n",
        "    self.num_layers = sum(layers)\n",
        "    self.inplanes = channels_per_block[0]\n",
        "    self.conv1 = conv3x3(3, channels_per_block[0])\n",
        "    self.bn1 = nn.BatchNorm2d(channels_per_block[0])\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.layers = []\n",
        "    for i, (num_channels, num_blocks) in enumerate(zip(channels_per_block,\n",
        "                                                       layers)):\n",
        "      self.layers.append(self._make_layer(block, num_channels, num_blocks,\n",
        "                                          stride=(1 if i == 0 else 2)))\n",
        "      self.add_module('layer%d' % i, self.layers[-1])\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    self.fc = nn.Linear(channels_per_block[-1], num_classes)\n",
        "\n",
        "    for _, m in sorted(self.named_modules()):\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    # Zero-initialize the last BN in each residual branch so that the residual\n",
        "    # branch starts with zeros, and each residual block behaves like an\n",
        "    # identity. This improves the model by 0.2~0.3% according to\n",
        "    # https://arxiv.org/abs/1706.02677\n",
        "    for _, m in sorted(self.named_modules()):\n",
        "      if isinstance(m, BasicBlock):\n",
        "        nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1):\n",
        "    downsample = None\n",
        "    if stride != 1:\n",
        "      downsample = nn.Sequential(\n",
        "        nn.AvgPool2d(1, stride=stride),\n",
        "        nn.BatchNorm2d(self.inplanes),\n",
        "      )\n",
        "\n",
        "    layers = []\n",
        "    layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "    self.inplanes = planes\n",
        "    for _ in range(1, blocks):\n",
        "      layers.append(block(planes, planes))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    x = self.avgpool(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Helper functions ###\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# function to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def visualise_model(model, num_images=4):\n",
        "    \"\"\"\n",
        "    Visualizes the model's predictions on a few test images.\n",
        "    \"\"\"\n",
        "\n",
        "    # get test images\n",
        "    dataiter = iter(testloader)\n",
        "    images, labels = next(dataiter)\n",
        "\n",
        "    # print images\n",
        "    imshow(torchvision.utils.make_grid(images))\n",
        "\n",
        "    # show ground truth labels\n",
        "    print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' \n",
        "                                    for j in range(num_images)))\n",
        "\n",
        "    # show predicted labels\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
        "                                for j in range(num_images)))\n",
        "    \n",
        "def train(model, epochs=1000):\n",
        "    \"\"\"\n",
        "    Training loop using fixed hyper-parameters. Returns the trained model. \n",
        "    \"\"\"\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    # PATH = './cifar_net.pth'\n",
        "    # torch.save(model.state_dict(), PATH)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em1rvDNynFVu"
      },
      "source": [
        "### Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKlhZfAXnDHa",
        "outputId": "e66501a7-1563-4610-b2bf-8c3b5ed28fca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "model = ResNet(BasicBlock, [2, 2, 2], [16, 32, 64]).to(device)\n",
        "\n",
        "### optional: print model ###\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non-Determinisism Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parameter Initialisation\n",
        "\n",
        "*\"When training a model, parameters without preset values are initialized randomly according to a given distribution, e.g. a zero-mean Gaussian with variance determined by the number of input connections to the layer (Glorot & Bengio, 2010; He et al., 2015)\".*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# todo: control random initialisation of parameters\n",
        "\n",
        "def param_init(seed):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Shuffling\n",
        "\n",
        "*\"In stochastic gradient descent, the gradient is approximated on a random subset of examples, commonly implemented by using small batches of data iteratively in a shuffled training dataset (Bottou, 2012). Shuffling may happen either once, before training, or in between each\n",
        "epoch of training, the variant we use in this work\".*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# todo: control randomness in data shuffling\n",
        "\n",
        "def data_shuffle():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Augmentation\n",
        "\n",
        "*\"A common practice, data augmentation refers to randomly altering each training example to artificially expand the training dataset (Shorten & Khoshgoftaar, 2019). For example, randomly flipping images encourages invariance to left/right orientation\".*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# todo: control randomness in data augmentation\n",
        "\n",
        "def data_augmentation():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stochastic Regularisation\n",
        "\n",
        "*\"Some types of regularization, such as Dropout (Srivastava et al., 2014), take the form of stochastic operations internal to a model during training. Other instances of this include DropConnect (Wan et al., 2013) and variable length backpropagation through time (Merity et al., 2017), among many others.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \n",
        "\n",
        "def stochastic_operations():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Low-level Operations (cuDNN)\n",
        "\n",
        "*\"Often underlooked, many libraries that deep learning frameworks are built on, such as cuDNN (Chetlur et al., 2014), typically run nondeterministically in order to increase the speed of their operations. This nondeterminism is small when evaluated in the context of a single operation — in one test we performed it caused an output difference of 0.003%. In the case of cuDNN, the library we test, it is possible to disable nondeterministic behavior at a speed penalty on the order of ∼15%. However, unlike other nondeterminism sources, it is not possible to “seed” this; it is only possible to turn it on or off.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# todo: seed cudnn\n",
        "\n",
        "def cuDNN():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# todo: change 1 bit in network\n",
        "\n",
        "def one_bit_param_init():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def control_non_determinism():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cUEV-EnIpmj5"
      },
      "outputs": [],
      "source": [
        "def pairwise_diagreement(model1, model2):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Train and test loop (outline only) ###\n",
        "\n",
        "def non_determinism_experiment(*args):\n",
        "\n",
        "    models = []\n",
        "\n",
        "    for seed in range(1, 101):\n",
        "\n",
        "        # Control for different aspects of non-determinism\n",
        "        model = control_non_determinism(seed)\n",
        "\n",
        "        # Train model\n",
        "        model = train(model)\n",
        "\n",
        "        # Save model for evaluation\n",
        "        models.append(model)\n",
        "\n",
        "    # Evaluate models\n",
        "\n",
        "\n",
        "    return accuracy_mean, accuracy_std, cross_entropy_mean, cross_entropy_std, pairwise_disagreement\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Recreating the table from the paper ###\n",
        "\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Run experiments and collect results\n",
        "results = []\n",
        "\n",
        "# Parameter initialisation\n",
        "accuracy_mean, accuracy_std, cross_entropy_mean, cross_entropy_std, pairwise_disagreement = non_determinism_experiment('param_init')\n",
        "results.append([\"Parameter Initialization\", \n",
        "                f\"{accuracy_mean:.2f} ± {accuracy_std:.2f}\", \n",
        "                f\"{cross_entropy_mean:.4f} ± {cross_entropy_std:.4f}\", \n",
        "                f\"{pairwise_disagreement:.1f}\"])\n",
        "\n",
        "# Data shuffling\n",
        "accuracy_mean, accuracy_std, cross_entropy_mean, cross_entropy_std, pairwise_disagreement = non_determinism_experiment('data_shuffle')\n",
        "results.append([\"Data Shuffling\", \n",
        "                f\"{accuracy_mean:.2f} ± {accuracy_std:.2f}\", \n",
        "                f\"{cross_entropy_mean:.4f} ± {cross_entropy_std:.4f}\", \n",
        "                f\"{pairwise_disagreement:.1f}\"])\n",
        "\n",
        "# Data augmentation\n",
        "accuracy_mean, accuracy_std, cross_entropy_mean, cross_entropy_std, pairwise_disagreement = non_determinism_experiment('data_augmentation')\n",
        "results.append([\"Data Augmentation\", \n",
        "                f\"{accuracy_mean:.2f} ± {accuracy_std:.2f}\", \n",
        "                f\"{cross_entropy_mean:.4f} ± {cross_entropy_std:.4f}\", \n",
        "                f\"{pairwise_disagreement:.1f}\"])\n",
        "\n",
        "# cuDNN\n",
        "accuracy_mean, accuracy_std, cross_entropy_mean, cross_entropy_std, pairwise_disagreement = non_determinism_experiment('cuDNN')\n",
        "results.append([\"cuDNN\", \n",
        "                f\"{accuracy_mean:.2f} ± {accuracy_std:.2f}\", \n",
        "                f\"{cross_entropy_mean:.4f} ± {cross_entropy_std:.4f}\", \n",
        "                f\"{pairwise_disagreement:.1f}\"])\n",
        "\n",
        "# Data shuffling + cuDNN\n",
        "accuracy_mean, accuracy_std, cross_entropy_mean, cross_entropy_std, pairwise_disagreement = non_determinism_experiment('data_shuffle', 'cuDNN')\n",
        "results.append([\"Data Shuffling + cuDNN\", \n",
        "                f\"{accuracy_mean:.2f} ± {accuracy_std:.2f}\", \n",
        "                f\"{cross_entropy_mean:.4f} ± {cross_entropy_std:.4f}\", \n",
        "                f\"{pairwise_disagreement:.1f}\"])\n",
        "\n",
        "# Data shuffling + data augmentation + cuDNN\n",
        "accuracy_mean, accuracy_std, cross_entropy_mean, cross_entropy_std, pairwise_disagreement = non_determinism_experiment('data_shuffle', 'data_augmentation', 'cuDNN')\n",
        "results.append([\"Data Shuffling + Aug. + cuDNN\", \n",
        "                f\"{accuracy_mean:.2f} ± {accuracy_std:.2f}\", \n",
        "                f\"{cross_entropy_mean:.4f} ± {cross_entropy_std:.4f}\", \n",
        "                f\"{pairwise_disagreement:.1f}\"])\n",
        "\n",
        "# Parameter initialisation + data shuffling + data augmentation + cuDNN \n",
        "accuracy_mean, accuracy_std, cross_entropy_mean, cross_entropy_std, pairwise_disagreement = non_determinism_experiment('param_init', 'data_shuffle', 'data_augmentation', 'cuDNN')\n",
        "results.append([\"All Nondeterminism Sources\", \n",
        "                f\"{accuracy_mean:.2f} ± {accuracy_std:.2f}\", \n",
        "                f\"{cross_entropy_mean:.4f} ± {cross_entropy_std:.4f}\", \n",
        "                f\"{pairwise_disagreement:.1f}\"])\n",
        "\n",
        "# Print the table\n",
        "headers = [\"Nondeterminism Source\", \"Accuracy SD (%)\", \"Cross-Entropy SD\", \"Pairwise Disagree (%)\"]\n",
        "print(tabulate(results, headers=headers, tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Additional sources of non-determinism or combinations ###"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOSisJFQ8FfAqjwcfzX03ue",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
